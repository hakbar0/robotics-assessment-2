{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1187ffce",
   "metadata": {},
   "source": [
    "# Welcome to Reinforcement Learning!\n",
    "By: Abdulrahman Altahhan\n",
    "\n",
    "**Unit 4 Learning outcomes**\n",
    "1. understand how to predict the value function for a policy\n",
    "1. understand how to control an agent by inferring its policy from an action-value function\n",
    "1. appreciate the different tabular methods that we develop in each lesson and their application\n",
    "1. apply RL to control an agent in a simple environment\n",
    "1. understand the idea of generalised policy iteration (GPI) that is utilised by many RL methods\n",
    "\n",
    "**Unit 5 Learning outcomes**\n",
    "1. understand n-steps methods and the trade-offs that n represents\n",
    "1. understand planning methods and how to build a model along the way by learning\n",
    "1. understand how to predict the value function for a policy using function approximation\n",
    "\n",
    "**Unit 6 Learning outcomes**\n",
    "1. understand how to control an agent by inferring its policy from an action-value function with function approximation\n",
    "1. apply RL to control an agent in a more complex environment representation\n",
    "1. apply RL to control an agent in Games\n",
    "1. apply RL to control a robot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52374cc9",
   "metadata": {},
   "source": [
    "## Introduction and overview of the units\n",
    "In this and consecutive units, we cover the main ideas of reinforcement learning. RL has gained much attention in recent years due to its unmatched ability to tackle difficult control problems with a minimal assumption about the setting and the environment an agent works in. As you have seen in earlier units, controlling an agent (simulated robot) is not trivial and can often require a specific setup and strong assumptions about its environment, making the corresponding solution difficult to attain or impractical in real scenarios. In RL, we try to minimise these assumptions and require that only the environment adheres to the Markov property. In simple terms, the Markov property assumes that inferring what to do (taking action) in a specific state can be fully specified by looking at this state and does not depend on other states.\n",
    "\n",
    "\n",
    "In RL, we deal with states, actions and rewards. State space is the space that the agent operates in, whether physical or virtual. The state can represent something specific in the environment such as the location of a robot, an agent configuration, or both. The actions are the set of decisions available for the agent. An RL agent's main aim is to arrive at a cohesive policy that allows it to achieve a specific goal. This policy can take a simple form of s → a, which means if the agent is in state s, then take action a. This type of policy is deterministic because the agent will definitely take action a, if it is in state s. Another type of policy that we can deal with is also stochastic policy. Stochastic policies take the form of p(a|s), which gives the probability of taking an action a | given that the agent is in state s. For such a policy, the agent draws from the set of available actions according to the conditional probability which we call its policy.\n",
    "\n",
    "The reward function can take the simple form of s →r, or (s,a) →r, which are interpreted as, if in state s then give reward r, or if in state s and applied action a give reward r, respectively. The more general form is the probabilistic one with the form of p(r|s,a), which is interpreted as given the agent is in state s and applied action a, what is the probability of giving it a reward r. Another conditional probability that we might want to deal with is p(s’|s,a), which is the probability of transiting to state s’ given the agent was in state s and applied action a. This is called the transition probability. Both the transition and reward probabilities can be inferred from a more general probability that specifies the dynamics of the environment. This conditional probability takes the form of p(s’,r|s,a), which is interpreted as the probability of transitioning to state s’ and obtaining reward r given that the agent was in state s and applied action a. We will deal mostly with these dynamics in the second lesson of this unit. Bear in mind, though, that obtaining the dynamics is generally difficult or intractable for most cases except for the simplest environment. Nevertheless, it is useful to understand the basic ideas of RL.\n",
    "\n",
    "The reward function is strongly linked to the task the agent tries to achieve. This is the minimal information provided for the agent to indicate whether it is on the right track. We need to be careful not to devise a complicated reward function as it is usually unnecessary. If we do so, *we* will solve the agent's problem. Instead, we would want the *agent* to solve the problem by utilising its reward function and interacting with its environment to gain experience and sharpen its decision-making. Improving its decision-making involves two things, evaluating its current policy and changing it in a way that will improve its performance. This is where the sum of rewards that an agent can collect while achieving a task plays a major role. In RL, we usually link the policy with maximising the sum of the rewards an agent can obtain while moving towards achieving a task. The reward can be negative; in this case, the agent will try to minimise the sum of negative rewards that it collects before terminating. The termination happens when the agent achieves the required task or has taken a specific number of steps, or has consumed pre-set computational resources. An example of a good simple reward we can use is giving a robot a reward of 1 when it reaches a goal location and 0 otherwise. Another example is giving a robot a negative reward of -1 for each step it takes before reaching the goal location. These rewards have their advantages, and we will study them in our exercises. the advantage of the first type is its simplicity, but it can take longer to converge. The advantage of the second type is that it provides intermediate information that the agent can immediately utilise to improve its policy while acting in the environment before reaching the goal location. The second type is specifically useful for online learning and when we want to alleviate the agent from having to change its starting position to cover all possible starts. The first type is useful for studying a learning algorithm's capabilities with minimum information.\n",
    "\n",
    "The sum of rewards from a specific state to the end of the task is called *the return*. \n",
    "Because we do not know how long the agent may take to achieve the task and to fairly maximise the rewards and allow for variability, we discount the rewards so that more recent rewards have more effect than later rewards. Nevertheless, we aim to maximise the rewards in the long run. To be more explicit, we call the sum of discounted rewards from the current state s to the end of the task, the return (that agent will obtain in the future- the whole idea is to predict these rewards and be able to collect as much as possible).\n",
    "It turns out that discounting is necessary for theoretical guarantees, but in practice, this can be ignored mostly for episodic tasks (with few exceptions). This leads us to distinguish between episodic tasks and continuous tasks. Episodic tasks are those that have a start and end, while continuous tasks are those that have a start but never end. We will deal mostly with episodic tasks. \n",
    "The function that specifies each state's expected return when the agent *follows a specific policy* is called the value function and is denoted as V(s). On the other hand, the function that specifies the expected return of the current state given that the agent will take a specific action a then follows a specific policy is the action-value function and is denoted as Q(s,a)\n",
    "\n",
    "Nevertheless, we can aim to improve the policy directly without relying on the expected return. These types of algorithms that do so are called policy-gradient algorithms and depends on approximation.\n",
    "\n",
    "We will largely deal with two types of algorithms, tabular algorithms, which use a tabular representation of the value function V and the action-value function Q. These will be covered in the first unit. In the consecutive units, we will cover the second type of algorithm that deals with function approximation, where representing the state and actions in a table is not tractable or not practical. In these algorithms, we generalise the tabular algorithms that we cover in the first unit to use the models we covered in machine learning such as linear regression models or neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ff6c2",
   "metadata": {},
   "source": [
    "## RL in context: advantages and challenges\n",
    "In our RL coverage you will notice that we do not do classical robotics and we believe it is not the way to go except possibly for industrial robots. So, we do not need motion planning or accurate trajectory calculations/kinematics to allow the robot to execute a task, we simply let it learn by itself how to solve the task and this is what reinforcement learning is about. In our coverage we also do not supervise or directly teach the robot, this type of interference is called imitation. \n",
    "\n",
    "This is all great but what about challenges. Obviously, we still have several challenges to this approach. One is the number of experiments required to learn the task which can be numerous, which expose the robot to the risk of wear and tear due to repetition and can be very lengthy and tedious on the human that is supervising the task. This can be partially overcome by starting in simulation and then move to real robot called a technique called [sim-to-real](https://ai.googleblog.com/2021/06/toward-generalized-sim-to-real-transfer.html) where we can employ GANs(generative adversarial neural networks).  The other challenge is the time required for training regardless whether it is in simulation or in real scenarios. \n",
    "\n",
    "\n",
    "The origin of these problems is actually the exploitation/exploration needed by RL algorithms which is in the heart of what we will be doing in all of our RL coverage. Reducing the amount of training required is important and remains an active area for research. One approach is via experience replay and the other is via eligibility traces as we shall see later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce76ea0",
   "metadata": {},
   "source": [
    "## Textbook\n",
    "The accompanying textbook of this and consecutive units is the Sutton Barto book Introduction to RL available online [here](http://incompleteideas.net/book/RLbook2020.pdf). Please note that we explain the ideas of RL from a practical perspective and not from a theoretical perspective which is already covered in the textbook.\n",
    "\n",
    "**list of symbols**\n",
    "- v0: denotes an initial value\n",
    "- θ: denotes a threshold\n",
    "- nS: denotes states dimension \n",
    "- nA: denotes actions dimension \n",
    "- nR: denotes rewards dimension\n",
    "- nU: denotes number of updates\n",
    "- goal: a terminal state\n",
    "\n",
    "- r: current step reward\n",
    "- s: current step state\n",
    "- a: current step action\n",
    "- rn: next step reward\n",
    "- sn: next step state\n",
    "- an: next step action\n",
    "\n",
    "- α: learning rate\n",
    "- ε: exploration rate\n",
    "- dα: decay factor for α\n",
    "- dε: decay factor for ε\n",
    "- G(t+1,t+n): the return between time step t+1 and t+n\n",
    "\n",
    "- Rs: is the sum of rewards of an episode\n",
    "- Ts: is the steps of a set of an episode\n",
    "- Es: is the errors (RMSE) of an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8c243",
   "metadata": {},
   "source": [
    "## Plan\n",
    "In this unit we cover the tabular solution methods, while approximate solution methods will be covered in the next unit.\n",
    "\n",
    "Tabular and approximate solution methods falls under two types of RL methods that we will attempt to deal with \n",
    "1. Prediction methods: AKA Policy Evaluation Methods that attempt to find the best estimate for the value-function or action-value function given a policy.\n",
    "2. Control methods:    AKA Policy Improvement Methods that attempt to find the best policy, often by starting from an initial policy then moving into a better and improved policy.\n",
    "\n",
    "Policy improvement methods:\n",
    "1. Methods that improve the policy via improving the action-value function\n",
    "2. Methods that improve the policy directly\n",
    "\n",
    "We start by assuming that the policy is stationary. This will help us to come up with algorithm that deals with predicting the value function (expected return) for the state space. Then we will move to the policy improvement methods; i.e. these methods that help us to compare and improve our policy with respect to other policies and move to a better policy when necessary. Then we move to the control case (policy iteration methods).\n",
    "Note that the guarantee that comes form the policy-improvement theorem no longer applies when we move from the table representation for the value function form small state space to parametric function approximation representation for large state space. This will encourage us to move to direct policy-improvement methods instead of improving the policy via improving the value-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad7971",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<!-- - Unit 1,2,3: Robotics Essentials\n",
    "    - Introduction\n",
    "    - Frame of reference, Twists and Wrenches\n",
    "    - kinematics and Filters\n",
    "    - SLAM\n",
    "    - Motion planning\n",
    "    - etc\n",
    "     -->\n",
    "     \n",
    "- **Unit 4: DP and MC Algorithms**\n",
    "    - [Lesson 1: K-Arm Bandit and Learning Q](Lesson1_MultiArmBandit.ipynb)\n",
    "    - [Lesson 2: Grid World Markov Decision Processes](Lesson2_GridWorldMDPs.ipynb)\n",
    "    - [Lesson 3: Dynamic Programming](Lesson3_DynamicProgramming.ipynb)\n",
    "    - [Lesson 4: Monte Carlo Methods](Lesson4_MonteCarloMethods.ipynb)\n",
    "    \n",
    "    \n",
    "- **Unit 5: Tabular Bootstrapping Algorithms**\n",
    "    - [Lesson 5: Temporal Learning Methods](Lesson5_TemporalDifferenceMethods.ipynb)\n",
    "    - [Lesson 6: n-step Methods](Lesson6_nStepsMethods.ipynb)\n",
    "    - [Lesson 7: Planning Methods](Lesson7_PlanningMethods.ipynb)\n",
    "    - [Lesson 8: Function Approximation for Prediction](Lesson8_ApproximatePrediction.ipynb)\n",
    "\n",
    "\n",
    "- **Unit 6: Function Approximation and Application**\n",
    "    - [Lesson 9: Function Approximation for Control](Lesson9_ApproximateControl.ipynb)\n",
    "    - [Lesson 10: Learning with Eligibility Traces](Lesson10_EligibilityTraces_Prediction_Control.ipynb)\n",
    "    - [Lesson 11: Reinforcement Learning Application on Robot Navigation](Lesson11_RL_Robot.ipynb)\n",
    "    - [Lesson 12: Reinforcement Learning Application on Games Agents](Lesson12_RL_Games.ipynb)\n",
    "    - [CARLA]() (optional)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a4dc4",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "We need to install a library that will help us to import classes and functions between different jupyter notebooks. This will relieve us from migrating the code to a separate .py file and importing it. Instead, we can import directly from another jupyter notebook. Alternatively, we can copy the classes and functions code to separate .py files and import them as needed. We should be mindful of what we are importing in each stage. Although nbimporter is not maintained it works fine for our needs since we create and explain classes code and their functionality simultaneously. The major issue with this library is that the error messages become hard to interpret as they mix up between the cell id and the line numbers, and if you change something in one notebook, you would need to restart the kernel in other notebooks that use it. If you need to debug extensively, you should pull off the code you want to import into a .py file. After installing nbimporter, you would need to restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2ee7a",
   "metadata": {},
   "source": [
    "Run the following cell to install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5029fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbimporter in c:\\users\\hasee\\anaconda3\\lib\\site-packages (0.3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04bd15f",
   "metadata": {},
   "source": [
    "**External Material**\n",
    "1. Lecture videos of reinforcement learning [course](https://www.youtube.com/watch?v=2pWv7GOvuf0) by Sliver D.,  from DeepMinds\n",
    "1. See notes provided in [here](https://spinningup.openai.com/)\n",
    "1. See RL implementation [Baselines](https://stable-baselines3.readthedocs.io/en/master/index.html)\n",
    "1. To appreciate the advantages of our RL robotics over classical robotics you might want to watch few videos of\n",
    "    1. Lecture videos on classical modern robotics [course](https://www.youtube.com/watch?v=jVu-Hijns70&list=PLggLP4f-rq02vX0OQQ5vrCxbJrzamYDfx) by Lynch K., from Northwestern Robotics.\n",
    "    1. Lecture videos on classical robotics [course](https://see.stanford.edu/course/cs223a) by  Khatib O., from Stanford."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
