{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b5076c",
   "metadata": {},
   "source": [
    "Author: Abdulrahman Altahhan, March 2023.\n",
    "\n",
    "The notebook use a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard python libraries (numpy, matplotlib etc.).\n",
    "Please note that you will need to take the consent from the author to use the code for research, commercially or otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d771eb6",
   "metadata": {},
   "source": [
    "# Lesson 11- RL Application on Games\n",
    "\n",
    "**Learning outcomes**\n",
    "1. understand how to create a simple wrapper for a Gym environment to take advantage of its provided functionality\n",
    "1. understand how to integrate our previous classes with Gym to combine them in a powerful way\n",
    "1. appreciate the intricacy of applying RL to different domains such as games and robotics\n",
    "1. build on previous concepts to come up with suitable and sometimes novel algorithms to solve a problem at hand\n",
    "1. understand how to combine deep reinforcement learning with deep learning to create a powerful framework that allows automatic agent learning by observation or self-play.\n",
    "1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence for difficult problems that involve image processing and reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3dd4a7",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f198ab1",
   "metadata": {},
   "source": [
    "Let us first install the gym environment and other dependencies including tensorflow and keras."
   ]
  },
  {
   "cell_type": "raw",
   "id": "32d3938c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#gym\n",
    "!pip3 install gym\n",
    "!pip3 install pygame\n",
    "!pip3 install pyglet\n",
    "!pip3 install ale-py\n",
    "!pip3 install autorom\n",
    "!pip3 install 'gym[atari]' gym\n",
    "#!pip install --upgrade 'gym[atari]'\n",
    "!pip3 install --upgrade gym\n",
    "!pip3 install autorom[accept-rom-license]\n",
    "!pip3 install pydot\n",
    "# deep learning\n",
    "!pip3 install -U pip\n",
    "!pip3 install -U setuptools\n",
    "!pip3 install tensorflow\n",
    "!pip3 install keras\n",
    "\n",
    "#other\n",
    "!pip3 install opencv-python\n",
    "#!pip3 install cv2 # might not work for early python versions\n",
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c2a82",
   "metadata": {},
   "source": [
    "Let us test if it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83901a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 show tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4004c04",
   "metadata": {},
   "source": [
    "You should be able to see the location of your tensorflow. If you cannot you might need to do: sudo nano ~/.bashrc and append the tensorflow path as follows (be mindful to the python version, yours might be > 3.6)\n",
    "\n",
    "export PYTHONPATH=/home/user/.local/lib/python3.6/site-packages:$PYTHONPATH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b93c014",
   "metadata": {},
   "source": [
    "We can also check if our GPU is defined as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77335402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59505760",
   "metadata": {},
   "source": [
    "To run this notebook on a remote Azure lab server check this [link](https://docs.microsoft.com/en-us/azure/lab-services/class-type-jupyter-notebook#template-virtual-machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5269c",
   "metadata": {},
   "source": [
    "**Reading**:\n",
    "The accompanying reading of this lesson is **chapter 16** of our text book available online [here](http://incompleteideas.net/book/RLbook2020.pdf). Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435ac17",
   "metadata": {},
   "source": [
    "This lesson deals with [gym openai](https://gym.openai.com/). OPenAI Gym provides a rich set of libraries and environments to try our algorithms on. These include the Atari games the DQN used in their Nature [paper](https://storage.googleapis.com/deepmind-media/DQN/DQNNaturePaper.pdf) to show that Deep Reinforcement Learning can supersede human performance on Atari games with the *same* generic model that is used in *all* of the presented games. \n",
    "\n",
    "This was a big and important breakthrough since, previously, RL applications stayed largely within simpler control environments and required, as it was customary at that time, manual feature extraction. So, when this paper showed that it is feasible to build an end-to-end RL model that can automatically extract useful features that can be used directly in an RL training algorithm without any human engineering or intervention, it was an important landmark in our field. Prior to that, there was the Tesauro Backgammon [TDGammon](https://en.wikipedia.org/wiki/TD-Gammon) as another success story which used a neural network to train a model to learn to play backgammon (see the chapter for more details). \n",
    "\n",
    "From that moment on, DeepMind went into a progressive trajectory of successful research and applications in the domain of deep reinforcement learning that reached an unprecedented level of beating the world champion in the game of Go see AlphaGo [paper](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ) and then moved on to create a generic architecture that is capable of self-playing to reach superhuman levels in chess, Go and shogi that they called AlphaZero. Contrary to AlphaGo, AlphaZero did not use any human knowledge to train the model, and it completely started from scratch and let the AI train itself by itself! see [here](https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go). \n",
    "\n",
    "Note that even the replay idea is an old idea in RL it just got rehashed and done a bit differently inside a buffer that allows us to conduct learning in mini-batches similar to what we do in supervised learning (since this was tried and tested by the ML community and it is proven to work very well with backpropagation).\n",
    "Yet, this idea was not new since other researchers have tried batches with RL. The paper's main flair is the impressive performance level that could be achieved via an adequately long period of training, a foot that has not been achieved before.\n",
    "Note that the replay buffer dictates the choice of an off-policy algorithm i.e. Q-learning, since the replayed experience is old and the agent will be learning from a policy different to the one it pursues.\n",
    "\n",
    "We expect this trajectory to continue and that RL with robotics will create the next wave of innovation that will hopefully change how we conduct our daily lives. We hope this will lead to positive changes and prosperity in the long run, but that does not prevent mistakes. You will tackle this ethical side in another module. For now, enjoy dealing with the revolutionary side of AI that will change the world!\n",
    "\n",
    "Ok, let us get started...!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce948a7e",
   "metadata": {},
   "source": [
    "# OpenAI Gym Classical Environment\n",
    "We first tackle classical environments in OpenAI Gym as useful training for dealing with the library. In particular, we will start with their mountain car environment. We have already created our own class for this problem in the previous lesson, however, here, we will inherit their class, and we will override their rendering mechanism to be able to embed its visualisation in our notebook. \n",
    "\n",
    "This is a useful exercise to familiarise ourselves with Gym and verify our findings in the previous lesson. Therefore, we will repeat some of the experiments we conducted in the previous lesson. No commentary is provided as it replicates previous experiments on our newly created class that depends on Gym.\n",
    "Note that we have already set up our classes in previous lessons to be ready to integrate easily with Gym. We will show this in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a037f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975f5ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.classic_control import MountainCarEnv\n",
    "from gym import wrappers\n",
    "import pygame\n",
    "from ale_py import ALEInterface\n",
    "ale = ALEInterface()\n",
    "\n",
    "from ale_py.roms import Breakout\n",
    "# ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94deabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from Lesson10_EligibilityTraces_Prediction_Control import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27c711",
   "metadata": {},
   "source": [
    "We start by showing you how to inherit from gym environment. We will do exactly what we did in the previous lesson regarding training a mountain car. This time we will utilise the openai gym environment rendering and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCar(MountainCarEnv):\n",
    "    def __init__(self, ntiles=8, **kw):   #  ω: position window, rd: velocity window\n",
    "        super().__init__(**kw)\n",
    "       \n",
    "        # constants                          \n",
    "        self.X0,  self.Xn  = -1.2, .5       # position range\n",
    "        self.Xv0, self.Xvn = -.07, .07      # velocity range\n",
    "        self.η = 3                          # we rescale by 3 to get the wavy valley/hill\n",
    "\n",
    "        # for render()\n",
    "        self.X  = np.linspace(self.X0,  self.Xn, 100)     # car's position\n",
    "        self.Xv = np.linspace(self.Xv0, self.Xvn, 100)    # car's speed\n",
    "        self.Y  = np.sin(self.X*self.η)\n",
    "        \n",
    "        # for state encoding (indexes)\n",
    "        self.ntiles  = ntiles\n",
    "        # number of states is nS*nSd but number of features is nS+nSd with an econdoing power of 2^(nS+nSd)>>nS*nSd!\n",
    "        self.nF = self.nS = 2*(self.ntiles+1)\n",
    "        self.nA = 3\n",
    "        # for compatability\n",
    "        self.Vstar = None\n",
    "        \n",
    "        # reset\n",
    "        self.x, self.xv = super().reset()\n",
    "\n",
    "        # figure setup\n",
    "        self.figsize = (17, 3)\n",
    "        #plt.gcf().set_size_inches(self.figsize[0], self.figsize[1])\n",
    "\n",
    "    def s(self, tilings=1):\n",
    "        return int(tilings*self.ntiles*(self.x  - self.X0 )/(self.Xn  - self.X0 ))\n",
    "    \n",
    "    def sv(self, tilings=1):\n",
    "        return int(tilings*self.ntiles*(self.xv - self.Xv0)/(self.Xvn - self.Xv0))\n",
    "\n",
    "    def reset(self):\n",
    "        self.x, self.xv = super().reset(seed=0)\n",
    "        return self.s_()\n",
    "        \n",
    "    def s_(self):\n",
    "        φ = np.zeros(self.nF)\n",
    "        φ[self.s()] = 1\n",
    "        φ[self.sv() + self.ntiles] = 1\n",
    "        return φ\n",
    "      \n",
    "    # for compatibility\n",
    "    def S_(self):\n",
    "        return np.eye(self.nF)\n",
    "    \n",
    "    def isatgoal(self):\n",
    "        return self.x>=self.Xn\n",
    "    \n",
    "    \n",
    "    def step(self,a):\n",
    "        obs, r, done, _ = super().step(a)\n",
    "        self.x, self.xv = obs[0], obs[1]\n",
    "        return self.s_(), r, done, {}\n",
    "\n",
    "    def render(self, visible=True, pause=0, subplot=131, animate=True, **kw):\n",
    "\n",
    "        if not visible: return\n",
    "        self.ax0 = plt.subplot(subplot)\n",
    "        self.figsize = (17, 3)\n",
    "        plt.gcf().set_size_inches(self.figsize[0], self.figsize[1])\n",
    "        plt.imshow(super().render(mode=\"rgb_array\"))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if animate: \n",
    "            clear_output(wait=True)\n",
    "            plt.show(); time.sleep(pause)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f06c66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time sarsa = Sarsa(env=MountainCar(ntiles=8), α=.1/8, episodes=45, seed=1, animate=True, **demoTR()).interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a00914",
   "metadata": {},
   "source": [
    "Note that we were only showing the end state in each episode and not the whole training process. We can make the process faster by not showing(animating) the training progress as we do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40781eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sarsa = Sarsa(env=MountainCar(ntiles=8), α=.1/8, episodes=45, seed=1, **demoTR()).interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb43049",
   "metadata": {},
   "source": [
    "No exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = Sarsa(env=MountainCar(ntiles=8), α=.3/8, ε=0, episodes=500, seed=1, plotT=True, plotR=True).interact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c869e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = Sarsa(env=MountainCar(ntiles=16), α=.3/8, ε=0, episodes=500, seed=1, plotT=True, plotR=True).interact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = Sarsa(env=MountainCar(ntiles=32), α=.3/8, ε=0, episodes=500, seed=1, plotT=True).interact()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MountainCarRuns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664bdd4",
   "metadata": {},
   "source": [
    "To run similar experiments that we did in the previous lesson we need to use the tiledMountainCar class.\n",
    "Below we restate the tiledMountainCar class that we developed in the previous lesson. No changes here except that it inherits now from the new MountainCar cvlass that uses gym MountainCarEnv class. We could have avoided this by putting tiledMountainCar in a class factory function. This is left for you as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dafec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tiledMountainCar(MountainCar):\n",
    "    def __init__(self, ntilings=1, **kw): #ntilings: is number of tiles\n",
    "        super().__init__(**kw)\n",
    "        self.ntilings = ntilings\n",
    "        self.dim = (self.ntilings, self.ntiles+2, self.ntiles+3) # the redundancy to mach the displacements of position(x) and velocity(xv)\n",
    "        self.nF = self.dim[0]*self.dim[1]*self.dim[2]\n",
    "\n",
    "\n",
    "    def inds(self):\n",
    "        s_tiling = self.s(self.ntilings)\n",
    "        sv_tiling = self.sv(self.ntilings)\n",
    "        \n",
    "        inds = []\n",
    "        for tiling in range(self.ntilings):\n",
    "            s  = (s_tiling  + 1*tiling )//self.ntilings\n",
    "            sv = (sv_tiling + 3*tiling )//self.ntilings\n",
    "            \n",
    "            inds.append((tiling,s,sv))\n",
    "        \n",
    "        return inds\n",
    "    \n",
    "    def s_(self):\n",
    "        φ = np.zeros(self.dim)\n",
    "        for ind in self.inds(): \n",
    "            φ[ind]=1\n",
    "        \n",
    "        return φ.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00078014",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in trange(5):\n",
    "    SarsaOnMountainCar(ntilings=2**n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897905c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MountainCarRuns(runs=10, env=IHTtiledMountainCar(ntilings=8,ntiles=8), label='with index hashed table for 8*8*8 tiles')\n",
    "MountainCarRuns(runs=10, env=tiledMountainCar(ntilings=8,ntiles=8), label='with 8*8*8 tiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6eb6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IHTtiledMountainCar(tiledMountainCar):\n",
    "    def __init__(self, iht_size=1024, **kw): # by default we have 8*8*8 (position tiles * velocity tiles * tilings)\n",
    "        super().__init__(**kw)\n",
    "        self.nF = iht_size\n",
    "\n",
    "\n",
    "    def s_(self):\n",
    "        φ = np.zeros(self.nF)\n",
    "        inds = np.where(super().s_()!=0)[0]\n",
    "        φ[inds%self.nF]=1\n",
    "        return φ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "MountainCarTilings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb365eb",
   "metadata": {},
   "source": [
    "As we can see we got identical results to the same experiments that we ran in the previous lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c93314",
   "metadata": {},
   "source": [
    "#  OpenAI Gym Atari Games Environments\n",
    "Now we are ready to move to our Atari environment. To be able to follow the logic it is better if you read the Nature paper since most tutorial follow the deep learning architecture that the paper presented. Note that the deep neural network architecture is just three CNNs, nevertheless the paper pioneered and showed the effectiveness of combing deep learning with reinforcement learning. The paper has few inventions such as the experience replay buffer and the batch training something which had not been done successfully before in RL in such a context. The main contribution is to prove that RL is generic enough to be applied to learn from images just as human learn to play a game. Note that the algorithm needed to be an off-policy because of the usage of the experience replay buffer. The use of experience replay means that the agent is learning its current policy from an old experience that stemmed from following an old policy (previous version of its current policy). \n",
    "\n",
    "\n",
    "Note that in terms of theory we are still lagging to prove convergence of such approaches. This is where we can be uncertain which shadow of doubt on the ethical controllability of our own creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ce51d",
   "metadata": {},
   "source": [
    "## Atari Gym Wrapper\n",
    "Let us now build our own wrapper for Atari game so that we are able to apply Deep Q-Learning on the screen pixels coming from the environment. We have equipped our environment with necessary video storing as well as with pre-processing and to be able to handle the frames of the Gym environment. Note that we are dealing with environments that do not perform skipping to avoid the stochasticity associated with randomly applying actions. Therefore, we had to do the frame skipping in the wrapper class. It is also sufficient to deal with grey scale images not RGB since this will reduce the processing demands on our machines. Feel free to experiments with the GymEnv class to change its underlying preprocessing and or its assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GymEnvi(Wrapper=gym.Wrapper):\n",
    "    \n",
    "    class GymEnvi(Wrapper):\n",
    "        def __init__(self, env_name='ALE/Pong-v5', seed=0,   # 'PongNoimgskip-v4'\n",
    "                     nimgs_skip=4, nimgs_stack=1, img_size=(84, 84), \n",
    "                     video=True, i=0, animate=False, saveimg=False):\n",
    "\n",
    "            if Wrapper==gym.Wrapper:\n",
    "                super().__init__(gym.make(env_name, render_mode='rgb_array'))\n",
    "                \n",
    "                self.nA = 3 if 'Pong' in env_name else self.action_space.n \n",
    "                self.env_name = env_name\n",
    "                # seeding the game for consistency when testing\n",
    "                # self.env.seed(seed)\n",
    "            \n",
    "            else: \n",
    "                self.env_name = 'grid'\n",
    "                self.env = self\n",
    "                self.nA = super.nA\n",
    "                \n",
    "            self.animate = animate\n",
    "            self.saveimg = saveimg\n",
    "            self.nimgs_skip = nimgs_skip      # how many imgs will be skipped to form one step\n",
    "            self.imgs_skip  = deque(maxlen=2)   # a buffer to store last 2 imgs that will take their max\n",
    "\n",
    "            self.nimgs_stack = nimgs_stack    # how many imgs will be put together to form a one state (we use first and last only)\n",
    "            self.img_size    = img_size       # (w,h)how much reduction will be applied on the original imgs\n",
    "            self.img_size_   = (*img_size, max(1,nimgs_stack)) # extended dimension of state space\n",
    "            self.imgs_stack  = deque(maxlen=nimgs_stack)\n",
    "\n",
    "\n",
    "            self.nS = 10 # for compatibility\n",
    "            self.i = i   # video number\n",
    "\n",
    "            # ineffective for compatibility \n",
    "            self.Vstar = None\n",
    "\n",
    "            # for rendering and video\n",
    "            self.ax0 = None\n",
    "            self.figsize = (20, 4)\n",
    "            self.video = video\n",
    "            self.video_imgs  = []\n",
    "            self.video_imgs_ = []\n",
    "\n",
    "        # self.s holds the latest img *after* preprocessing (state/observation seen by the agent)\n",
    "        def preprocess(self, img):\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img = cv2.resize(img, self.img_size) / 255\n",
    "            self.s = np.expand_dims(img, -1)\n",
    "            return self.s \n",
    "\n",
    "        # self.img holds the latest img *before* preprocessing (will be used for videos)\n",
    "        def step(self, a):\n",
    "            if a and 'Pong' in self.env_name: a+=1 # suitable for pong only\n",
    "            self.a = a\n",
    "            self.r = 0\n",
    "            # skip 4 imgs (apply same action) and stack last 2 imgs then take their max\n",
    "            for i in range(self.nimgs_skip):\n",
    "                img, r, self.done, _ = self.env.step(a)\n",
    "                self.r += r\n",
    "                self.imgs_skip.append(img)\n",
    "                if self.done: break\n",
    "\n",
    "            img = self.preprocess(np.stack(self.imgs_skip).max(axis=0))\n",
    "            self.imgs_stack.append(img)\n",
    "\n",
    "            # stack only the first and last imgs to save computation and to convey movement direction to the model\n",
    "            img_inds = [0,-1] if self.nimgs_stack>1 else [0]\n",
    "            self.img = np.dstack([self.imgs_stack[i] for i in img_inds ])\n",
    "\n",
    "            return self.img, self.r, self.done, {}\n",
    "\n",
    "        def reset(self):\n",
    "            self.imgs_skip.clear()\n",
    "            self.imgs_stack.clear()\n",
    "\n",
    "            img = self.env.reset()\n",
    "            self.imgs_skip.append(img)\n",
    "            img = self.preprocess(img)\n",
    "\n",
    "            # now stack the same img n times for consistency with step()\n",
    "            for _ in range(self.nimgs_stack):\n",
    "                self.imgs_stack.append(img)\n",
    "            img_inds = [0,-1] if self.nimgs_stack>1 else [0]\n",
    "            self.img = np.dstack([self.imgs_stack[i] for i in img_inds ])\n",
    "\n",
    "            return self.img \n",
    "\n",
    "        #------------------------------------------render ✍️-------------------------------------------------\n",
    "        def render(self, visible=True, pause=0, subplot=131,  animate=True, **kw):\n",
    "\n",
    "            if not visible: return\n",
    "\n",
    "            self.ax0 = plt.subplot(subplot)\n",
    "            plt.gcf().set_size_inches(self.figsize[0], self.figsize[1])\n",
    "\n",
    "            # saving it as a video if needed, note that we render only at the last few episode\n",
    "            if self.video and animate:\n",
    "                self.video_imgs.append(self.img)\n",
    "                plt.axis('off')\n",
    "                # create the video at the end of the set of episodes\n",
    "                if self.done:\n",
    "                    for obsv in self.video_imgs: self.video_imgs_.append([plt.imshow(obsv, cmap=cm.Greys_r, animated=True)])\n",
    "                    anim = animation.ArtistAnimation(self.ax0.figure, self.video_imgs_, interval=50, blit=True, repeat_delay=1000)\n",
    "                    anim.save('atari%d.mp4'%self.i)\n",
    "            img_inds = [0, int(self.nimgs_stack/2), -1] if self.nimgs_stack > 2 else [0]\n",
    "            img = np.dstack([self.imgs_stack[i] for i in img_inds])\n",
    "\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            if animate:\n",
    "                clear_output(wait=True)\n",
    "                plt.show()\n",
    "                time.sleep(pause)\n",
    "\n",
    "    return GymEnvi\n",
    "\n",
    "GymEnv = GymEnvi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acee98d",
   "metadata": {},
   "source": [
    "Let us create some handy function to play an Atari games and observe the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e96ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(ep=1, env=GymEnv(nimgs_stack=1), render=True): # try nframes_stack=5 it is fun!\n",
    "    for ep in range(ep):\n",
    "        env.reset()\n",
    "        done=False\n",
    "        for _ in range(50):\n",
    "            s,r, done, _ = env.step(randint(3))\n",
    "            if render: env.render(pause=0)\n",
    "    print(s.shape)\n",
    "    return s\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9658777",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c51a8c",
   "metadata": {},
   "source": [
    "# RL with Deep Learning \n",
    "It is time to extend our basic MRP class to handle function approximation using deep neural networks.\n",
    "Note that in their paper DeepMinds trained for 200M frames, we set the max_t_exp (used in stop_exp() function) to 2M due to hardware limitation. We can also make the stop_exp() tied to R-star that is specific to the game under consideration. For example, in Pong, we can set this to 18. This means that on average (for the last 100 or 200 games/episodes), the opponent could only score *only a max of 3* goals, and our agent scores the wining *21* goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ca0eb",
   "metadata": {},
   "source": [
    "### Buffer Implementation\n",
    "It is better to implement the buffer as queue because it guarantees an O(1) complexity for append() and pop() and it is preferred over the list which gives us a O(n) for adding and retrieving an item. In Python we can utilise the double queue structure which gives us the flexibility to add and retrieve from both ends of the queue. Below, we show a quick example. Note that the buffer will be overwritten when. the number of items exceeds its length. This is useful for us because we just want the buffer to overwritten with new experience after it s full and to be kept updated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0220fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "buffer = deque(maxlen=5)\n",
    "buffer.append(1)\n",
    "buffer.append(2)\n",
    "buffer.append(3)\n",
    "buffer.append(4)\n",
    "print(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.append(5)\n",
    "buffer.append(6)\n",
    "buffer.append(7)\n",
    "print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0148c",
   "metadata": {},
   "source": [
    "### Sampling form the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.sample(buffer,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdb7d0",
   "metadata": {},
   "source": [
    "### Buffer with Complex Element (Tuples)\n",
    "Let us assume that we have a set of tuples each consists of (s,a,sn). In this case we can add these tuples as is. Below we show an example, we have represented actions as integers but states/observations as string to help identifying them visually, but bear in mind that they are going to be a more complex entities such as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6035788",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = deque(maxlen=4)\n",
    "buffer.append(('2',1,'3'))\n",
    "buffer.append(('3',2,'4'))\n",
    "buffer.append(('4',2,'5'))\n",
    "buffer.append(('5',1,'4'))\n",
    "\n",
    "print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16722f99",
   "metadata": {},
   "source": [
    "Now in order to sample we can directly sample from the buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = random.sample(buffer,3)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f9c5f",
   "metadata": {},
   "source": [
    "However, the above is not useful, usually we want to place all the actions and states and next states in their own list to feed them as a batch into a neural network. To put all states and actions together each in its own list we can use zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45131ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in zip(*batch): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4327ed9",
   "metadata": {},
   "source": [
    "We can also convert them into a numpy array directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e31bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.array(i) for i in zip(*batch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = deque(maxlen=4)\n",
    "buffer.append(('2',1,'3'))\n",
    "buffer.append(('3',2,'4'))\n",
    "buffer.append(('4',2,'5'))\n",
    "buffer.append(('5',1,'4'))\n",
    "\n",
    "print(buffer)\n",
    "samples = [np.array(item) for item in zip(*sample(buffer,3))]\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec8255",
   "metadata": {},
   "source": [
    "## Deep MRP\n",
    "In this class we implement the basic functionality for dealing with creating, saving and loading deep learning models. In addition, we make these models the default functions used to obtain the value function via self.V_.\n",
    "We also adjust the stope_exp criterion so that the algorithm stops when a specific averaged reward is achieved or when a specific *total* number of steps (self.t_ not self.t) have been elapsed. This means also that we free ourselves from the notion of an episode, so our model can run as many episodes as it takes to achieve this total number of steps. We still can assign episodes=x to store metrics for last y episodes where y<x.\n",
    "Note that nF is usually used in the Env(ironment) class but feature extraction is embedded the model itself in deep learning model so it is defined in the Deep_MRP class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13566ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "200000%(int(2e6)*.1)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_MRP(MDP()):\n",
    "    def __init__(self, \n",
    "                 env=GymEnv(), \n",
    "                 γ=0.99,\n",
    "                 nF=512, \n",
    "                 R_star=None, \n",
    "                 last=40,\n",
    "                 buffer_size=10000, \n",
    "                 batch_size=32, \n",
    "                 max_t_exp=int(1e6),\n",
    "                 load_weights=False,\n",
    "                 print_=True,\n",
    "                 \n",
    "                 **kw):\n",
    "        \n",
    "        super().__init__(env=env, γ=γ, last=last, print_=print_, **kw)\n",
    "        self.nF           = nF # feature extraction is integrated within the deep learning model not the env\n",
    "        self.R_star       = R_star\n",
    "        self.buffer_size  = buffer_size\n",
    "        self.batch_size   = batch_size\n",
    "        self.load_weights_= load_weights\n",
    "        self.max_t_exp    = max_t_exp # used to stop learning\n",
    "\n",
    "    \n",
    "    def init(self):\n",
    "        self.vN = self.create_model('V')                      # create V deep network\n",
    "        if self.load_weights_: self.load_weights(self.vN,'V') # from earlier training proces\n",
    "        self.vN.summary()\n",
    "        \n",
    "        self.V = self.V_\n",
    "\n",
    "    #-------------------------------------------Deep model related---------------------------\n",
    "    def create_model(self, net_str):\n",
    "        print(f'model for {net_str} network is being loaded from disk........!')\n",
    "        x0 = Input((84,84,1))#self.env.frame_size_)\n",
    "        x = Conv2D(32, 8, 4, activation='relu')(x0)\n",
    "        x = Conv2D(64, 4, 2, activation='relu')(x)\n",
    "        x = Conv2D(64, 3, 1, activation='relu')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(self.nF, 'relu')(x)\n",
    "        x = Dense(1 if net_str=='V' else self.env.nA)(x) \n",
    "        model = Model(x0, x)\n",
    "        model.compile(Adam(self.α), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def load_weights(self, net, net_str ):\n",
    "        print(f'weights for {net_str} network are being loaded from disk........!')\n",
    "        loaded_weights = net.load_weights(net_str)\n",
    "        loaded_weights.assert_consumed()\n",
    "\n",
    "    def save_weights(self):\n",
    "        print(f'weights for V network are being saved to disk........!')\n",
    "        self.vN.save_weights('V')\n",
    "\n",
    "    #------------------------------------- value related 🧠-----------------------------------\n",
    "    def V_(self, s):\n",
    "        if len(s.shape)!=4: \n",
    "            # this needs to be tested to make sure it give us the required dim\n",
    "            return self.vN.predict(np.expand_dims(s, 0))[0]  # prediction for one state for εgreedy to work well\n",
    "        return np.copy(self.vN.predict(s))  # prediction for a batch of states, we copy to avoid auto-grad issues\n",
    "\n",
    "    # ------------------------------------ experiments related --------------------------------\n",
    "    # overriding: we do not specify a preset number of episodes to stop the experiments\n",
    "    def stop_exp(self):  \n",
    "        \n",
    "        if self.ep and self.R_star is not None and self.Rs[:self.ep].mean()>= self.R_star: \n",
    "            print(f'target reward is achieved in {self.t_} steps!')\n",
    "            return True\n",
    "        \n",
    "        if self.t_ > self.max_t_exp:\n",
    "            print(f'max steps of {self.t_} is reached and training is stopped, weights are being saved!')\n",
    "            self.save_weights()\n",
    "            return True\n",
    "        \n",
    "        # save model's weights at least 10 times during the life of the agent\n",
    "        if (self.t_+1)%int(.1*self.max_t_exp)==0: self.save_weights()\n",
    "        return False\n",
    "    \n",
    "    # stop_ep must be overriden because otherwise it will stope when max_t is reached\n",
    "    def stop_ep(self, done): return done\n",
    "    \n",
    "    #-------------------------------------------buffer related----------------------------------\n",
    "    def allocate(self):\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def store_(self, s=None,a=None,rn=None,sn=None,an=None, done=None, t=0):\n",
    "        self.buffer.append((s, a, rn, sn, done))\n",
    "\n",
    "    def sample(self):\n",
    "        # sample a set of batch_size tuples (each tuple has 5 items) without replacement \n",
    "        # zip the tuples into one tuple of 5 items and convert each item into a np array \n",
    "        # of size batch_size   \n",
    "        samples = [np.array(experience) for experience in zip(*sample(self.buffer, self.batch_size))]\n",
    "\n",
    "        # generate a set of indices handy for filtering, to be used in online()\n",
    "        inds = np.arange(self.batch_size)\n",
    "        \n",
    "        return samples, inds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647cf09f",
   "metadata": {},
   "source": [
    "## Deep MDP\n",
    "Now we create the Deep_MDP class which implements policy related functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_MDP(MDP(Deep_MRP)):\n",
    "    def __init__(self, \n",
    "                 ε=1.0,  \n",
    "                 εmin=0.01, \n",
    "                 εT=200000,    # linear decay by default\n",
    "                 t_qNn=1000,   # update the target network every t_qNn steps\n",
    "                 create_vN=False,\n",
    "                 **kw):\n",
    "        \n",
    "        super().__init__(ε=ε, εmin=εmin, εT=εT, **kw) \n",
    "        self.t_qNn = t_qNn \n",
    "        self.create_vN = create_vN\n",
    "\n",
    "    \n",
    "    def init(self):\n",
    "        self.create_vN: super().init()                        # to create also vN, suitable for actor-critic\n",
    "        \n",
    "        self.qN  = self.create_model('Q')                     # create main policy network\n",
    "        self.qNn = self.create_model('Q')                     # create target network to estimate Q(sn)\n",
    "        if self.load_weights_: self.load_weights(self.qN,'Q') # from earlier training proces\n",
    "        self.qNn.set_weights(self.qN.get_weights())\n",
    "        \n",
    "        self.qN.summary()\n",
    "        \n",
    "        self.Q = self.Q_\n",
    "\n",
    "    def save_weights(self):\n",
    "        self.create_vN: super().save_weights()\n",
    "        print(f'weights for Q network are being saved to disk........!')\n",
    "        self.qN.save_weights('Q')\n",
    "    #------------------------------------- policies related 🧠-----------------------------------\n",
    "    def Q_(self, s):\n",
    "        if len(s.shape)!=4: \n",
    "            return self.qN.predict(np.expand_dims(s, 0))[0]  # prediction for one state for εgreedy to work well\n",
    "        return np.copy(self.qN.predict(s))  # prediction for a batch of states, we copy to avoid auto-grad issues\n",
    "    \n",
    "    def Qn(self, sn):\n",
    "        return self.qNn.predict(sn)    \n",
    "    #-------------------------------------- 🔍 conditions -----------------------------------------\n",
    "    \n",
    "    def online_cond(self):  return len(self.buffer) >= self.buffer_size\n",
    "    def target_cond(self):  return self.t_ % self.t_qNn==0\n",
    "        \n",
    "    #-------------------------------------- 🧠 deep nets updates ----------------------------------- \n",
    "    def update_before(self, *args): pass\n",
    "    def update_after(self, *args): pass\n",
    "    def update_online_net(self): pass    \n",
    "    \n",
    "    # update the target network every now and then\n",
    "    def update_target_net(self):\n",
    "        if self.target_cond(): \n",
    "            print('assigning weights for target network.....')\n",
    "            self.qNn.set_weights(self.qN.get_weights())\n",
    "    \n",
    "\n",
    "    #------------------------------------- 🌖 online learning --------------------------------------       \n",
    "    def online(self, *args):\n",
    "        # make sure the buffer is full before doing any update or sampling\n",
    "        if self.online_cond(): \n",
    "            self.update_before(*args) \n",
    "            self.update_target_net()\n",
    "            self.update_online_net()\n",
    "            self.update_after(*args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7580b3ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61160822",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cec2d",
   "metadata": {},
   "source": [
    "Note that we need to set ε here otherwise it will be set by default to .1 in the parent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6548a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Deep_MDP):\n",
    "    def __init__(self, α=1e-4, **kw): \n",
    "        print('--------------------- 🧠  DQN is being set up 🧠 -----------------------')\n",
    "        super().__init__(**kw)\n",
    "        self.α = α\n",
    "        \n",
    "    #------------------------------- 🌖 online learning ---------------------------------\n",
    "    # update the online network in every step using a batch\n",
    "    def update_online_net(self):\n",
    "        # sample a tuple batch: each componenet is a batch of items \n",
    "        #(ex. s is a set of states, a is a set of actions)\n",
    "        (s, a, rn, sn, dones), inds = self.sample() \n",
    "\n",
    "        # obtain the action-values estimation from the two networks \n",
    "        # and make sure target is 0 for terminal states\n",
    "        Qs = self.Q(s)\n",
    "        Qn = self.Qn(sn); Qn[dones] = 0\n",
    "\n",
    "        # now dictate what the target should have been as per the Q-learning update rule\n",
    "        Qs[inds, a] = self.γ*Qn.max(1) + rn\n",
    "        self.qN.fit(s, Qs, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa329a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with the Grid states as images and learn from them to navigate it\n",
    "%time deepqlearn = DQN(env=GymEnv(), episodes=5, max_t_exp=20000, εT=500, buffer_size=10000, batch_size=32).interact() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827af982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# play pong and take the mean of the last n episodes but will keep going until R.mean() reaches R_star\n",
    "%time deepqlearn = DQN(env=GymEnv(),R_star=10, episodes=50, max_t_exp=200000, εT=50000, buffer_size=10000, batch_size=32).interact() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa4c7d",
   "metadata": {},
   "source": [
    "## Double DQN Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(DQN):\n",
    "    def __init__(self, α=1e-4, **kw):\n",
    "        print('----------- 🧠 Double DQN is being set up 🧠 ---------------------')\n",
    "        super().__init__(**kw)\n",
    "        self.α = α\n",
    "    #--------------------------- 🌖 online learning -----------------------------\n",
    "    def update_online_net(self):\n",
    "        # sample a tuple batch: each componenet is a batch of items \n",
    "        #(ex. s is a set of states, a is a set of actions) \n",
    "        (s, a, rn, sn, dones), inds = self.sample()\n",
    "        # obtain the action-values estimation from the two networks \n",
    "        # and make sure target is 0 for terminal states\n",
    "        Qs = self.Q(s)\n",
    "        Qn = self.Qn(sn); Qn[dones] = 0\n",
    "\n",
    "        # now dictate what the target should have been as per the *Double* Q-learning update rule\n",
    "        # this is where the max estimations are decoupled from the max action selections\n",
    "        an_max = self.Q(sn).argmax(1)\n",
    "        Qs[inds, a] = self.γ*Qn[inds, an_max] + rn\n",
    "        self.qN.fit(s, Qs, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play pong and take the mean of the last n episodes but will keep going until R.mean() reaches R_star\n",
    "%time doubledeepqlearn = DDQN(env=GymEnv(), episodes=20, buffer_size=10000, batch_size=32).interact() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d46eb6",
   "metadata": {},
   "source": [
    "## Extracting Features via Auto-Encoders\n",
    "In this section we show how to use the latent variables of an auto-encoder in order to extract useful features from the frames grabbed from the games. This will allow us to apply previously covered algorithms that can be applied on linear models. For example we can take the latent variables and use them as the input for true online TD($\\lambda$). \n",
    "Refer to keras [auto-encoder](https://www.tensorflow.org/tutorials/generative/autoencoder) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e5cea",
   "metadata": {},
   "source": [
    "### Build a dataset from a game\n",
    "Let us collect a dataset from our game by assigning the max_t_exp to be as large as the buffer_size so that the games stops when the buffer is almost full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76222fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(env=GymEnv(), buffer_size=41000, max_t_exp=40000).interact() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43829ab4",
   "metadata": {},
   "source": [
    "Now extract the data frames from the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba143aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = [np.array(experience) for experience in zip(*dqn.buffer)]\n",
    "frames = experience[0]\n",
    "x_train, x_test = train_test_split(frames, test_size=0.3, random_state=42)\n",
    "print('training data size', x_train.shape)\n",
    "print('testing data size', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006a235",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "In this lesson you saw how to deal with a continuous state space using function approximation and how to apply previous concepts on a more difficult control problems. We have built a wrapper class that allowed us to take advantage of the environments provided by OpenAI Gym library. We have duplicated what we have done in the previous lesson in order to 1. examine that our previous environment worked well, 2. see an example of how to deal with OpenAI Gym environment. \n",
    "\n",
    "You have also seen how to combine deep learning with reinforcement learning to create a powerful model that is capable of learning from watching a game. This is really interesting since it opens up the possibility for enormous applications where an agent can watch and learn to arrive to a complex behaviour that allows it to accomplish a task or win a competition. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2f0ca",
   "metadata": {},
   "source": [
    "# Unit's conclusion\n",
    "This lesson concludes our unit where we have studied important formulation of RL that we use a function approximation to represent the stare space which can be continuous and infinite.\n",
    "\n",
    "We only expect that the interest is going to continue to grow and that RL with robotics will create the next wave of innovation that will hopefully change the way we conduct our daily lives. We hope that this will lead to positive changes and to prosperity in the long run but that does not prevent mistakes. You will tackle this ethical side in another module, for now enjoy dealing with revolutionary side of AI that will change the world!\n",
    "\n",
    "Congratulations on completing this last unit on RL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9543643",
   "metadata": {},
   "source": [
    "## Discussion and Activity\n",
    "Read the following classic Nips [paper](https://deepmind.com/research/publications/2019/playing-atari-deep-reinforcement-learning) and Nature [paper](https://storage.googleapis.com/deepmind-media/DQN/DQNNaturePaper.pdf) and discuss it in the discussion forum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97e81a",
   "metadata": {},
   "source": [
    "## Extra Resources\n",
    "There are plenty of videos and resources which you can search online for.\n",
    "- You may want to follow the following [tutorial](https://www.tensorflow.org/agents/tutorials/1_t_DQNnutorial#environment) which shows how to deal with tensorflow on cart pole but the input is not the frames.\n",
    "- You may then follow this [tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#hyperparameters-and-utilities) which uses deep learning on the frames of a cart pole problem.\n",
    "\n",
    "- You may find it useful to watch this [video](https://www.youtube.com/watch?v=a5XbO5Qgy5w) for a Keras tutorial with RL, or this [video](https://www.youtube.com/watch?v=NP8pXZdU-5U) for a pytorch tutorial with RL. \n",
    "- You may want to have a look at some [tutorials](https://neptune.ai/blog/best-reinforcement-learning-tutorials-examples-projects-and-courses).\n",
    "\n",
    "- You may find see this series of talks about the [future of AI](https://www.bbc.co.uk/sounds/play/m001216j) by Stuart Russell interesting.\n",
    "- If you are intersted in self-driving cars, then:\n",
    "    - See the [ALVIN](https://papers.nips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf) paper for an early stage vehicle road control neural network, it is an early precursor of the current road systems that control autonomous vehicle. A lot of the new systems retain some similarities with this systems. \n",
    "    - See this [video](https://youtu.be/2KMAAmkz9go) of the history of this system. \n",
    "    - See this [video](https://youtu.be/sRxaMDDMWQQ) lecture of the topic autonomous vehicle driving.\n",
    "\n",
    "- **Carla Autonomous Vehicle Simulation**\n",
    "    - See this [video](https://www.youtube.com/watch?v=pONr1R1dy88) for Carla simulation tutorial 1\n",
    "    - See this [video](https://www.youtube.com/watch?v=om8klsBj4rc) for Carla simulation tutorial 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138629c",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "1. try to apply the same concept on other simple environments provided by Gym such as the acrobot.\n",
    "\n",
    "2. apply DQN on another Atari game such as SpaceInvaders or Breakout and report the score that you got in the discussion forum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b340c",
   "metadata": {},
   "source": [
    "## Challenge++\n",
    "1. check the implementation of the deep network in the [tutorial](https://keras.io/examples/rl/actor_critic_cartpole/) and try to integrate it into the provided infrastructure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
